---
title: "Polygenic Risk of T1D in Hispanic People"
author: "Tim Vigers & Laura Pyle"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    number_sections: yes
    theme: readable
    toc: yes
    toc_depth: 2
    toc_float: yes
---

```{r setup, include=FALSE}
library(ggplot2)
library(pROC)
library(caret)
library(parallel)
knitr::opts_chunk$set(echo = FALSE)
home_dir = ifelse(.Platform$OS.type != "unix","T:/",
                  "~/Dropbox/Work/Kimber Simmons/GWAS/Data_Cleaned/biobank_analysis/imputed/")
knitr::opts_knit$set(root.dir = home_dir)
```

# Traditional PGRS Approach

## Number of significant SNPs by p-value cutoff

```{r}
logistic_res = read.delim("./plink2.PHENO1.glm.logistic",header = T)
logistic_res = logistic_res[logistic_res$TEST == "ADD",]
```

There are `r sum(logistic_res$P<1e-30,na.rm=T)` significant SNPs at the $p<10^{-30}$ level, `r sum(logistic_res$P<1e-25,na.rm=T)` significant SNPs at the $p<10^{-25}$ level, and `r sum(logistic_res$P<1e-20,na.rm=T)` significant SNPs at the $p<10^{-20}$ level. So, I generated a PRS from the SNPs significant at the $p<10^{-30}$ level.

# Lasso results

```{r}

```

# Test and Training Split

```{r}
pheno = read.table("./merged_imputed_qc.fam")
train_index = createDataPartition(pheno$V6,p=0.75,list=F)

```

```{r eval=FALSE}
pruned = read.table("merged_imputed_qc.prune.in")
sample = read.plink("./merged_imputed_qc.bed",
                 "./merged_imputed_qc.bim",
                 "./merged_imputed_qc.fam",select.snps=pruned$V1)
```

# Questions 
1. plink's lasso function requires an $h^2$ estimate. Lam et al. table suggests 0.6 is reasonable, is this right?
![](/Users/timvigers/Dropbox/Work/Kimber Simmons/GWAS/Data_Cleaned/biobank_analysis/imputed/h2_table.png)
2. How do we validate these scores with external data? The AUC is really good for all models, which is concerning from an over-fitting point of view.