---
title: "JDRF Models"
author: "Tim Vigers & Laura Pyle"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dpi = 600)
knitr::opts_knit$set(root.dir = "Z:/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Greg Forlenza/JDRF")
library(Hmisc)
library(arsenal)
library(skimr)
library(knitr)
library(Epi)
library(glmnet)
library(tidyverse)
```

```{r import data,echo=FALSE}
source("C:/Users/timbv/Documents/GitHub/BDC-Code/Greg Forlenza/JDRF/data_import.R")
# Exclude participants after 115 and those who never started AM
exclude <- data$record_id[which(data$gyl_timepoint==0 & is.na(data$automode_start))]
data <- data %>% filter(record_id <= 115,!(record_id %in% exclude))
```

```{r data cleaning,echo=FALSE,message=FALSE}
# Binary endpoint and predictors
# The primary endpoint of interest will be percent time in closed loop at 1 year 
# captured as a continuous percentage for the 2 weeks prior to the 1 year clinical 
# follow up visit, and will be converted to a binary variable with ≥60% representing 
# successful AP use and <60% representing unsuccessful AP use.
df <- left_join(data %>% group_by(record_id) %>% 
                  filter(gyl_timepoint.factor =="12 Months" | 
                           gyl_timepoint.factor =="9 Months") %>%
                  filter(row_number() == n()) %>%
                  summarise(am_final = time_am), 
                data %>% filter(grepl("baseline",redcap_event_name)),
                by = "record_id")
# Remove duplicates
df <- df[-c(which(duplicated(df$record_id))-1),]
# PAID
df$c_paid_total <- 
  apply(df[,which(colnames(df)=="c_paid1"):
             which(colnames(df)=="c_paid20")],1,
        function(x){mean(5-x)*25})
df$ya_paid_total <- 
  apply(df[,which(colnames(df)=="ya_paid1"):
             which(colnames(df)=="ya_paid20")],1,
        function(x){sum(x)*1.25})
# HFS behave
df$c_hfs_behave_total <- 
  apply(df[,which(colnames(df)=="c_hfs_behave1"):
             which(colnames(df)=="c_hfs_behave10")],1,
        function(x){sum(x)})
df$ya_hfs_behave_total <- 
  apply(df[,which(colnames(df)=="ya_hfs_b_behave1"):
             which(colnames(df)=="ya_hfs_b_behave15")],1,
        function(x){sum(x)})
# HFS worry
df$c_hfs_worry_total <- 
  apply(df[,which(colnames(df)=="c_hfs_worry11"):
             which(colnames(df)=="c_hfs_worry25")],1,
        function(x){sum(x)})
df$ya_hfs_worry_total <- 
  apply(df[,which(colnames(df)=="ya_hfs_b_worry16"):
             which(colnames(df)=="ya_hfs_b_worry33")],1,
        function(x){sum(x)})
# Add 1 month variables, select relevant columns
df <- left_join(df,data %>% 
                  filter(gyl_timepoint.factor == "Month 1/ Training F/U") %>%
                  mutate(m1_sensor_wear = sensor_wear,
                         m1_time_am = time_am,
                         m1_tir = sensor_70_180) %>%
                  select(record_id,m1_sensor_wear,m1_time_am,m1_tir),
                by = "record_id") %>%
  select(record_id,am_final,hba1c,c_paid_total,ya_paid_total,
         c_hfs_behave_total,ya_hfs_behave_total,
         c_hfs_worry_total,ya_hfs_worry_total,
         m1_sensor_wear,m1_time_am,m1_tir,
         demographics_t1d_duration,demographics_age,
         demographics_sex.factor,demographics_insurance.factor,
         demographics_race.factor,demographics_ethnicity.factor,
         demographics_cgmhx.factor,demographics_pumphx.factor)
# Binary outcome
df$success <- as.factor(ifelse(df$am_final >= 60,1,0))
levels(df$success) <- c("Failure","Success")
# Categorical variables
df$Age <- df$demographics_age
df$demographics_age <- cut(df$demographics_age,c(5,13,18,Inf),right = T,
                           labels = c("6 - 13","14 - 18","18+"))
levels(df$demographics_race.factor) = 
  c("Non-white","Non-white","Non-white","Non-white","White","Non-white","Non-white")
levels(df$demographics_cgmhx.factor) = 
  c("<= 6 months","<= 6 months","6+ months","6+ months","6+ months","6+ months",
    "<= 6 months")
levels(df$demographics_pumphx.factor) = 
  c("<= 6 months","<= 6 months","6+ months","6+ months","6+ months","6+ months",
    "<= 6 months")
```

# Model selection

## Lasso

The lasso [1] is a shrinkage method of variable selection, similar to ridge regression. Shrinkage methods impose a penalty on regression coefficients, causing unimportant variables go to 0. The advantage of this approach is that "shrinkage methods are more continuous, and don’t suffer as much from high variability" [2] compared to discrete processes like backwards selection. 

The lasso solves this equation:

![](/Users/timvigers/GitHub/BDC-Code/Greg Forlenza/JDRF/lasso.png)

but basically the only thing you need to understand is that the parameter $\lambda$ controls the amount of shrinkage applied. The higher $\lambda$ is, the more aggressively the coefficients are shrunk. So, it's important to find the optimal $\lambda$ value that results in an interpretable model without too many variables, but with good prediction error compared to the full model. 

Luckily, the glmnet package [3] does pretty much all the work for us! It partitions the data set into 10 equal sub-samples, reserves one for validation and uses the remaining 9 to build the model. This is repeated 10 times so that each subset is used to validate the model once. Within each "fold," multiple values of $\lambda$ are tested. Then, the average error and standard deviation of the error are computed over the folds. Finally, it picks the largest value of $\lambda$ (i.e. the smallest model) such that the error is within one SE of the minimum error. 

### Lasso issues

Using the lasso for model selection requires complete data, and the survey variables have a lot of missing data. The sample size of YA participants is too small to use the lasso, but none of the surveys are associated with the outcome based on simple logistic models of the form:

$$
logit(success) = \beta_0+\beta_1*paid+\beta_2*worry+\beta_3*behave + \epsilon
$$

Because of all this, surveys were excluded from the lasso model selection, in order to keep the data in one cohort and to avoid missing data problems.

### Without interaction terms

```{r without interact,echo=FALSE,warning=FALSE}
# Remove surveys
lasso <- df %>% select(-ya_paid_total,-ya_hfs_behave_total,-ya_hfs_worry_total,
                       -c_paid_total,-c_hfs_behave_total,-c_hfs_worry_total)
# Lasso
# Make model matrix and outcome vector
form <- as.formula("success ~ hba1c + m1_sensor_wear + m1_time_am + m1_tir + 
demographics_t1d_duration + demographics_age + demographics_sex.factor + 
demographics_insurance.factor + demographics_race.factor + 
demographics_ethnicity.factor + demographics_cgmhx.factor + 
demographics_pumphx.factor")
cc <- lasso[complete.cases(lasso),]
x <- model.matrix(form,cc)[,-1]
y <- cc$success
# Fit
fit <- glmnet(x,y,family = "binomial")
plot(fit,xvar = "lambda")
# Cross validation for lambda
cv.fit <- cv.glmnet(x, y, alpha=1, family = "binomial")
coef(cv.fit)
```
As lambda increases, the coefficients are all shrunk to 0. In the table above, the numbers themselves are not important. What matters is whether or not a coefficient was shrunk to 0 or not. The same applies to the lasso with interaction terms included.

### With interaction terms

Because sensor wear and time in automode at month 1 are correlated, only sensor wear was included in further model selection.

```{r with interact,echo=FALSE,warning=FALSE}
# Lasso
# Make model matrix and outcome vector
form <- as.formula("success ~ demographics_age*hba1c + 
demographics_age*m1_sensor_wear + demographics_age*m1_tir + 
demographics_age*demographics_sex.factor + 
demographics_age*demographics_ethnicity.factor + 
demographics_age*demographics_pumphx.factor")
cc <- lasso[complete.cases(lasso),]
x <- model.matrix(form,cc)[,-1]
y <- cc$success
# Fit
fit <- glmnet(x,y,family = "binomial")
plot(fit,xvar = "lambda")
# Cross validation for lambda
cv.fit <- cv.glmnet(x, y, alpha=1, family = "binomial")
coef(cv.fit)
```

## Re-fitting the model

After applying the lasso to determine which variables are kept in the model, we re-fit a standard logistic regression (because the lasso estimates are biased by the shrinkage parameter).

### Issues

Unfortunately, several categorical variables selected by the lasso caused fitted probabilities numerically 0 or 1. In other words, there are levels of the interaction terms where there are either no successes or no failures. I assumed that the lasso would somehow pick up on this and would shrink all the categorical variables coefficients to 0, but this was clearly not the case (and in retrospect a silly assumption).

Also, there is unfortunately not enough variability in pump history or ethnicity to get accurate estimates, since 73/90 (~ 80%) participants are not Hispanic with 6+ months of pump history. See the large coefficients and standard errors in the model summary below.

```{r log model,echo=FALSE,results='asis'}
full_model = as.formula("success ~ demographics_ethnicity.factor + 
interaction(demographics_age,demographics_sex.factor) + 
interaction(demographics_age,demographics_pumphx.factor)")
full_mod = glm(full_model,family = "binomial",df)

t = tableby(full_model,df)
kable(summary(t),caption = "Group Ns")
```

```{r echo=FALSE}
no_int_model = as.formula("success ~ hba1c + m1_sensor_wear + m1_tir + 
demographics_ethnicity.factor + demographics_age + demographics_sex.factor + 
demographics_pumphx.factor")

no_int_mod = glm(no_int_model,family = "binomial",df)
kable(summary(no_int_mod)$coefficients,caption = "Bad model summary")
```

The tables above are simply to demonstrate how these issues look in the variable counts and model results.

## Comparing AM Use to Sensor Wear

```{r echo=FALSE}
# Model formulas
model_sens_form <- 
  as.formula("success ~ scale(hba1c,scale=F) + scale(m1_sensor_wear,scale=F) +
             scale(m1_tir,scale=F) + demographics_age + demographics_sex.factor")
model_am_form <- 
  as.formula("success ~ scale(hba1c,scale=F) + scale(m1_time_am,scale=F) +
             scale(m1_tir,scale=F) + demographics_age + demographics_sex.factor")
# Fit
model_sens <- glm(model_sens_form,family = "binomial",df)
model_am <- glm(model_am_form,family = "binomial",df)
# Compare
kable(AIC(model_am,model_sens))
r_sens <- ROC(form = model_sens_form,data = df,MX=F,MI=F,PV=F,plot = "ROC",
              main = "Sensor Wear Model")
r_am <- ROC(form = model_am_form,data = df,MX=F,MI=F,PV=F,plot = "ROC",
            main = "AM Use Model")
```

Technically the sensor wear model is better by both AIC and ROC AUC, but both appear to be good models. For now I'll continue using AM time since that is what Stanford collected.

## Comparing 1 month variables to 3 month variables

```{r echo=FALSE}
# Get three month data
three_month <- data %>% 
  filter(gyl_timepoint.factor == "3 Months") %>%
  mutate(m3_sensor_wear = sensor_wear,
         m3_time_am = time_am,
         m3_tir = sensor_70_180) %>%
  select(record_id,m3_sensor_wear,m3_time_am,m3_tir)
# Add to df
df <- left_join(df,three_month,by = "record_id")
# Compare 1 month to 3 month
model1 <- 
  as.formula("success ~ scale(hba1c,scale=F) + scale(m1_time_am,scale=F) +
  scale(m1_tir,scale=F) + demographics_age + demographics_sex.factor")
model3 <- 
  as.formula("success ~ scale(hba1c,scale=F) + scale(m3_time_am,scale=F) + 
  scale(m3_tir,scale=F) + demographics_age + demographics_sex.factor")
# Fit
# Same observations
temp <- df %>% filter(!is.na(m1_tir) & !is.na(m3_tir))
model_m1 <- glm(model1,family = "binomial",temp)
model_m3 <- glm(model3,family = "binomial",temp)
# Compare
kable(AIC(model_m1,model_m3))
r_sens <- ROC(form = model1,data = df,MX=F,MI=F,PV=F,plot = "ROC",
              main = "M1 Model")
r_am <- ROC(form = model3,data = df,MX=F,MI=F,PV=F,plot = "ROC",
            main = "M3 Model")
```

Interestingly, the month 1 model is better than the month 3 model. There are 75 observations with month 1 data and 63 with month 3 data, so is it possible that dropout is already having an effect? Also I have been using the 90 participants with data at 9 or 12 months, but that could mean we're excluding people who dropped out entirely. Is that okay?

## Results

```{r stanford,echo=FALSE,message=FALSE,warning=FALSE}
# Read in
stanford <- read.csv("./Data_Cleaned/stanford.csv",
                     na.strings = c("","NA","na","N/A"),stringsAsFactors = F)
# Format variables to match BDC data
num_vars <- c("Baseline.A1c","Auto.Mode...1","Time.in.range.....1")
stanford[,num_vars] <- lapply(stanford[,num_vars], as.numeric)
# Age groups
stanford$demographics_age <- 
  cut(stanford$Age,c(5,13,18,Inf),right = T,
      labels = c("6 - 13","14 - 18","18+"))
# Insurance - combine checkboxes
stanford$insurance <- 
  ifelse(stanford$Insurance.Type..choice.Private.HMO. == "Checked" | 
           stanford$Insurance.Type..choice.Private.PPO. == "Checked" |
           stanford$Insurance.Type..choice.Other.Private. == "Checked","Private",
         ifelse(stanford$Insurance.Type..choice.CCS. == "Checked" | 
                  stanford$Insurance.Type..choice.MediCal. == "Checked" |
                  stanford$Insurance.Type..choice.MediCare. == "Checked" |
                  stanford$Insurance.Type..choice.Other.government. == "Checked",
                "Public","Other"))
# Race
stanford$race <- 
  ifelse(stanford$Race..choice.White. == "Checked","White","Non-white")
# Get variables from appropriate time points
stan_df <- stanford %>% group_by(Participant.ID) %>%
  summarise(hba1c = Baseline.A1c[Event.Name=="670G Start"],
            m3_time_am = Auto.Mode...1[Event.Name=="3 month visit"],
            m3_tir = Time.in.range.....1[Event.Name=="3 month visit"],
            demographics_age = demographics_age[Event.Name=="Baseline"],
            demographics_sex.factor = Gender[Event.Name=="Baseline"],
            demographics_t1d_duration = 
              as.numeric(T1D.Duration[Event.Name=="Baseline"]),
            demographics_insurance.factor = insurance[Event.Name=="Baseline"],
            demographics_race.factor = race[Event.Name=="Baseline"],
            demographics_ethnicity.factor = Ethnicity[Event.Name=="Baseline"],
            Age = Age[Event.Name=="Baseline"]) %>%
  filter(demographics_sex.factor != "Trans female")
# Correct factor levels
stan_df$demographics_sex.factor <- as.factor(stan_df$demographics_sex.factor)
stan_df$demographics_insurance.factor <- 
  factor(stan_df$demographics_insurance.factor,
         levels = levels(df$demographics_insurance.factor))
stan_df$demographics_race.factor <- 
  factor(stan_df$demographics_race.factor,
            levels = levels(df$demographics_race.factor))
stan_df$demographics_ethnicity.factor <- 
  factor(stan_df$demographics_ethnicity.factor)
levels(stan_df$demographics_ethnicity.factor) <- 
  levels(df$demographics_ethnicity.factor)
# Get am use at final timepoint
success <- stanford %>% group_by(Participant.ID) %>%
  filter(Event.Name == "12 month visit" | 
           Event.Name =="9 month visit") %>%
  filter(row_number() == n()) %>%
  summarise(am_final = Auto.Mode...1)
# Combine covariates and success dfs
stan_df <- left_join(stan_df,success,by = "Participant.ID")
stan_df$success <- as.factor(ifelse(stan_df$am_final >= 60,"Success","Failure"))
# Cohort
stan_df$cohort <- "Stanford"
colnames(stan_df)[1] <- "record_id"
df$cohort <- "BDC"
t1_df <- full_join(df,stan_df)
```

### Table 1a: Descriptive Characteristics by Cohort

```{r table 1,echo=FALSE,results='asis'}
# Table 1 design formula. Visually non-normal variables tested with K-W, categorical
# variables with < 5 in any cell tested with Fisher's exact
t1_form <- "cohort~Age+kwt(am_final)+hba1c+kwt(m1_sensor_wear)+kwt(m3_sensor_wear)+kwt(m1_time_am)+kwt(m3_time_am)+m1_tir+m3_tir+kwt(demographics_t1d_duration)+success+demographics_age+demographics_sex.factor+fe(demographics_insurance.factor)+demographics_race.factor+fe(demographics_ethnicity.factor)"

t1 <- tableby(as.formula(t1_form),data = t1_df[!is.na(t1_df$success),])
summary(t1,labelTranslations = 
          list(am_final = "AM Use at Final Visit",hba1c = "Baseline HbA1c",
               m1_sensor_wear = "M1 Sensor Wear",m3_sensor_wear = "M3 Sensor Wear",
               m1_time_am = "M1 Time in AM",m3_time_am = "M3 Time in AM",
               m1_tir = "M1 TIR",m3_tir = "M3 TIR",
               success = "Success",
               demographics_t1d_duration = "T1D Duration",
               demographics_age = "Age Group",demographics_sex.factor = "Sex",
               demographics_insurance.factor = "Insurance",
               demographics_race.factor = "Race",
               demographics_ethnicity.factor = "Ethnicity",
               demographics_cgmhx.factor = "CGM Hx.",
               demographics_pumphx.factor = "Pump Hx."),
        pfootnote=T)
```

### Table 1b: Descriptive Characteristics by Cohort and Sucess

```{r table 1b,echo=FALSE,results='asis'}
# Table 1 design formula. Visually non-normal variables tested with K-W, categorical
# variables with < 5 in any cell tested with Fisher's exact
t1_form <- "interaction(cohort,success)~Age+kwt(am_final)+hba1c+kwt(m1_sensor_wear)+kwt(m3_sensor_wear)+kwt(m1_time_am)+kwt(m3_time_am)+m1_tir+m3_tir+kwt(demographics_t1d_duration)+demographics_age+demographics_sex.factor+fe(demographics_insurance.factor)+demographics_race.factor+fe(demographics_ethnicity.factor)"

t1 <- tableby(as.formula(t1_form),data = t1_df[!is.na(t1_df$success),])
summary(t1,labelTranslations = 
          list(am_final = "AM Use at Final Visit",hba1c = "Baseline HbA1c",
               m1_sensor_wear = "M1 Sensor Wear",m3_sensor_wear = "M3 Sensor Wear",
               m1_time_am = "M1 Time in AM",m3_time_am = "M3 Time in AM",
               m1_tir = "M1 TIR",m3_tir = "M3 TIR",
               success = "Success",
               demographics_t1d_duration = "T1D Duration",
               demographics_age = "Age Group",demographics_sex.factor = "Sex",
               demographics_insurance.factor = "Insurance",
               demographics_race.factor = "Race",
               demographics_ethnicity.factor = "Ethnicity",
               demographics_cgmhx.factor = "CGM Hx.",
               demographics_pumphx.factor = "Pump Hx."),
        pfootnote=T)
```

### Final model (1 month variables)

```{r m1 time_am,echo=FALSE}
final_model <- 
  as.formula("success ~ scale(hba1c,scale=F) + 
  scale(m1_time_am,scale=F) + scale(m1_tir,scale=F) + 
  demographics_age + demographics_sex.factor")

final_mod = glm(final_model,family = "binomial",df)
t = broom::tidy(final_mod,conf.int = T,exponentiate=T)
t$term[2:nrow(t)] = c("Baseline HbA1c","M1 AM %","M1 TIR",
                      "Baseline Age 14 - 18","Baseline Age 18+","Male")
t[,2:ncol(t)] <- lapply(t[,2:ncol(t)],function(x){round(x,4)})

r = ROC(form = final_model,data = df,MX=F,MI=F,PV=F,plot = "ROC",main = "M1 Model")
kable(t)
```

```{r m1 sensor,echo=FALSE}
final_model <- 
  as.formula("success ~ scale(hba1c,scale=F) + 
  scale(m1_sensor_wear,scale=F) + scale(m1_tir,scale=F) + 
  demographics_age + demographics_sex.factor")

final_mod = glm(final_model,family = "binomial",df)
t = broom::tidy(final_mod,conf.int = T,exponentiate=T)
t$term[2:nrow(t)] = c("Baseline HbA1c","M1 Sensor Wear %","M1 TIR",
                      "Baseline Age 14 - 18","Baseline Age 18+","Male")
t[,2:ncol(t)] <- lapply(t[,2:ncol(t)],function(x){round(x,4)})

r = ROC(form = final_model,data = df,MX=F,MI=F,PV=F,plot = "ROC",main = "M1 Model")
kable(t)
```

### Final model (3 month variables)

```{r m3 time am model,echo=FALSE}
final_model <- 
  as.formula("success ~ scale(hba1c,scale=F) + 
  scale(m3_time_am,scale=F) + scale(m3_tir,scale=F) + 
  demographics_age + demographics_sex.factor")

final_mod = glm(final_model,family = "binomial",df)
t = broom::tidy(final_mod,conf.int = T,exponentiate=T)
t$term[2:nrow(t)] = c("Baseline HbA1c","M3 AM %","M3 TIR",
                      "Baseline Age 14 - 18","Baseline Age 18+","Male")
t[,2:ncol(t)] <- lapply(t[,2:ncol(t)],function(x){round(x,4)})

r = ROC(form = final_model,data = df,MX=F,MI=F,PV=F,plot = "ROC",main = "M3 Model")
kable(t)
```

```{r m3 sensor model,echo=FALSE}
final_model <- 
  as.formula("success ~ scale(hba1c,scale=F) + 
  scale(m3_sensor_wear,scale=F) + scale(m3_tir,scale=F) + 
  demographics_age + demographics_sex.factor")

final_mod = glm(final_model,family = "binomial",df)
t = broom::tidy(final_mod,conf.int = T,exponentiate=T)
t$term[2:nrow(t)] = c("Baseline HbA1c","M3 Sensor Wear %","M3 TIR",
                      "Baseline Age 14 - 18","Baseline Age 18+","Male")
t[,2:ncol(t)] <- lapply(t[,2:ncol(t)],function(x){round(x,4)})

r = ROC(form = final_model,data = df,MX=F,MI=F,PV=F,plot = "ROC",main = "M3 Model")
kable(t)
```

The coefficients above must be exponentiated in order to interpret them. So, the interpretation is that for each 1-unit increase in sensor wear at month 1, the odds of success increase $e^\beta$ = `r exp(t$estimate[t$term == "M3 Sensor Wear"])` times. Continuous variables have been mean-centered, so the intercept is interpreted as the average odds of success for a participant with average values of continuous variables and 0 (reference group) for all categorical.

### Validation

```{r test model,echo=FALSE}
# Model formula
final_model <- 
  as.formula("success ~ scale(hba1c,scale=F) + scale(m3_time_am,scale=F) + 
  scale(m3_tir,scale=F) + demographics_age + demographics_sex.factor")
# Fit
model_stanford <- glm(final_model,family = "binomial",stan_df)
# Results
t = broom::tidy(model_stanford,conf.int = T,exponentiate=T)
t$term[2:nrow(t)] = c("Baseline HbA1c","M3 AM %","M3 TIR",
                      "Baseline Age 14 - 18","Baseline Age 18+","Male")
t[,2:ncol(t)] <- lapply(t[,2:ncol(t)],function(x){round(x,4)})
kable(t)
# ROC
r = ROC(form = final_model,data = stan_df,MX=F,MI=F,PV=F,plot = "ROC",
        main = "Validation Cohort")
```

# References

1. 	Tibshirani R. Regression Shrinkage and Selection Via the Lasso. J R Stat Soc Ser B Methodol. 1996;58(1):267-288. doi:10.1111/j.2517-6161.1996.tb02080.x

2. 	Hastie T, Tibshirani R, Friedman JH. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer; 2009

3. 	Friedman J, Hastie T, Tibshirani R. Regularization Paths for Generalized Linear Models via Coordinate Descent. J Stat Softw. 2010;33(1). doi:10.18637/jss.v033.i01
